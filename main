import os
from collections import defaultdict
import math
import numpy as np
import pandas as pd

inverted_index = defaultdict(list)

path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\dokimi"
os.chdir(path)
words_set = set()

for file in os.listdir(path):
    file_path = os.path.join(path, file)
    with open(file_path, 'r') as folder:
        text = folder.read()
        dictionary = text.split()
        words_set = words_set.union(set(dictionary))
        # Create a dictionary to store word counts for each document
        count = {}

        for word in dictionary:
            count[word] = count.get(word, 0) + 1

        # Update the inverted index with word counts for the current document
        for word, count in count.items():
            inverted_index[word].append((file, count))

#print(words_set)

n_docs = len([f for f in os.listdir(path)])  # Number of documents in the corpus
n_words_set = len(words_set)  # Number of unique words in the

words_list = list(words_set)
df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=words_list)

# Compute Term Frequency (TF) for each document
for i, file in enumerate(os.listdir(path)):
    file_path = os.path.join(path, file)
    with open(file_path, 'r') as folder:
        text = folder.read()
        words = text.split()  # Words in the document
        total_words = len(words)
        for w in words:
            df_tf[w][i] = df_tf[w][i] + (1 / total_words)




csv_file_path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\tf.csv"

# Save the DataFrame to a CSV file
df_tf.to_csv(csv_file_path, index=False)





