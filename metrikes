import os
from collections import defaultdict
import math
import numpy as np
import pandas as pd
import sys
from matplotlib import pyplot as plt
from matplotlib.offsetbox import AnchoredText
inverted_index = defaultdict(list)

path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\docs"
os.chdir(path)
words_set = set()

document_count = {}  # Dictionary to store the count of documents each word appears in

for file in os.listdir(path):
    file_path = os.path.join(path, file)
    with open(file_path, 'r') as folder:
        text = folder.read()
        dictionary = text.split()
        words_set = words_set.union(set(dictionary))
        # Create a dictionary to store word counts for each document
        count = {}

        for word in dictionary:
            count[word] = count.get(word, 0) + 1

        # Update the inverted index with word counts for the current document
        for word, count in count.items():
            inverted_index[word].append((file, count))

                # Update the document count for the current word
            if word in document_count:
                document_count[word].add(file)
            else:
                document_count[word] = {file}

#print(inverted_index)


# Calculate IDF for each word and store it in a dictionary
idf_values = {}
for word, documents in document_count.items():
    idf = math.log10((len([f for f in os.listdir(path)])) / len(documents))
    idf_values[word] = idf

# Create a DataFrame with IDF values
idf_df = pd.DataFrame.from_dict(idf_values, orient='index', columns=['IDF'])

# Now you have a DataFrame with the IDF values for each word
print(idf_df)
csv_file_path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\idf.csv"

# Save the DataFrame to a CSV file
idf_df.to_csv(csv_file_path, index=False)

# Create a dictionary to store TF values for each word in each document
tf_values = {}
for word, occurrences in inverted_index.items():
    for document, count in occurrences:
        if document in tf_values:
            tf_values[document][word] = 1 + math.log10(count)
        else:
            tf_values[document] = {word: 1 + math.log10(count)}

# Create a DataFrame where rows are document titles and columns are words with their TF values
tf_df = pd.DataFrame.from_dict(tf_values, orient='index')
tf_df.fillna(0, inplace=True)  # Fill missing values with 0

# Now you have a DataFrame with TF values calculated as 1 + log(term_freq) for each word in each document
print(tf_df)

# Iterate through the TF and IDF DataFrames to calculate TF-IDF
tfidf_df = tf_df.copy()  # Create a copy of the TF DataFrame

# Multiply each TF value by the corresponding IDF value
for column in tfidf_df.columns:
    tfidf_df[column] = tfidf_df[column] * idf_df.loc[column, 'IDF']

# Now, tfidf_df contains the TF-IDF values
print(tfidf_df)

csv_file_path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\tfidf.csv"

# Save the DataFrame to a CSV file
tfidf_df.to_csv(csv_file_path, index=False)


query_path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\Queries_20"

# Read questions from the file
with open(query_path, 'r') as file:
    questions = file.readlines()


# Create a list of words from the inverted index
all_words = list(inverted_index.keys())

# Create a DataFrame with zeros
df = pd.DataFrame(0, index=range(len(questions)), columns=all_words)

# Iterate through questions and update the DataFrame
for i, question in enumerate(questions):
    words = question.upper().split()
    for word in words:
        if word in inverted_index:
            df.at[i, word] = 1

# Now, df is the DataFrame where each row represents a question, and columns represent words with values 1 or 0
print(df)
csv_file_path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\query.csv"

# Save the DataFrame to a CSV file
df.to_csv(csv_file_path, index=False)

idf_multiplied_df = df.multiply(idf_df['IDF'], axis=1)

print(idf_multiplied_df)

csv_file_path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\queryweights.csv"

# Save the DataFrame to a CSV file
idf_multiplied_df.to_csv(csv_file_path, index=False)

# Calculate the Euclidean norm for each row
euclidean_norms = np.linalg.norm(tfidf_df.values, axis=1)

# Create a DataFrame with the Euclidean norm values
euclidean_norms_df = pd.DataFrame({'Euclidean Norm': euclidean_norms}, index=tfidf_df.index)

# Now, euclidean_norms_df contains the Euclidean norm values for each row in tfidf_df
print(euclidean_norms_df)

# Calculate the Euclidean norm for each row
euclidean_norms_idf = np.linalg.norm(idf_multiplied_df.values, axis=1)

# Create a DataFrame with the Euclidean norm values
euclidean_norms_idf_df = pd.DataFrame({'Euclidean Norm': euclidean_norms_idf}, index=idf_multiplied_df.index)

# Now, euclidean_norms_idf_df contains the Euclidean norm values for each row in idf_multiplied_df
print(euclidean_norms_idf_df)

dot_products = np.dot(idf_multiplied_df.values, tfidf_df.values.T)

# Create a DataFrame with the dot product values
dot_product_df = pd.DataFrame(dot_products, columns=tfidf_df.index, index=idf_multiplied_df.index)

# Now, dot_product_df contains the dot product values for each pair of rows between idf_multiplied_df and tfidf_df
print(dot_product_df)

result_df = euclidean_norms_idf_df.values @ euclidean_norms_df.values.T

# Create a DataFrame with the result
result_df = pd.DataFrame(result_df, index=euclidean_norms_idf_df.index, columns=euclidean_norms_df.index)

# Now, result_df contains the result of multiplying euclidean_norms_idf_df with the transpose of euclidean_norms_df
print(result_df)

division_result_df = dot_product_df.div(result_df)


# Sort columns by converting column names to integers
division_result_df = division_result_df[sorted(division_result_df.columns, key=lambda x: int(x))]

# Save the sorted DataFrame to a CSV file
csv_file_path2 = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\cosine_similarity.csv"
division_result_df.to_csv(csv_file_path2, index=False)
print(division_result_df)
def sort_queries_all_docs(df):
    sorted_queries = pd.DataFrame(index=df.index, columns=['Sorted_Doc_IDs'])

    for query in df.index:
        sorted_values = df.loc[query].sort_values(ascending=False)
        sorted_doc_ids = list(sorted_values.index)

        sorted_queries.loc[query] = [sorted_doc_ids]

    return sorted_queries

# Sorting queries based on cosine similarity
sorted_queries_all_docs_df = sort_queries_all_docs(division_result_df)

# Display the result
print(sorted_queries_all_docs_df)

# Read relevant documents from file
relevant_documents_path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\Relevant_20"
relevant_documents_dict = {}

# Read relevant documents from file
with open(relevant_documents_path, 'r') as rel_file:
    for query_id, line in enumerate(rel_file):
        rel_docs = [f"{int(doc):05d}" for doc in line.strip().split()]
        relevant_documents_dict[query_id] = rel_docs

top_k = 20
sorted_queries_all_docs_df['Sorted_Doc_IDs'] = sorted_queries_all_docs_df['Sorted_Doc_IDs'].apply(lambda x: x[:top_k])

# Now, 'Top_K_Doc_IDs' contains the top k elements for each row
print(sorted_queries_all_docs_df)
# Convert all elements in the 'Sorted_Doc_IDs' column to integers
sorted_queries_all_docs_df['Sorted_Doc_IDs'] = sorted_queries_all_docs_df['Sorted_Doc_IDs'].apply(lambda x: sorted([int(doc_id) for doc_id in x]))
print(sorted_queries_all_docs_df)
precision_list = []
recall_list = []

# Assuming sorted_queries_all_docs_df['Sorted_Doc_IDs'] contains lists of document IDs
# Replace it with the actual column name in your DataFrame
for i, doc_ids in enumerate(sorted_queries_all_docs_df['Sorted_Doc_IDs']):
    retrieved_count = 0
    precision_list_query = []  # List to store precision for each document ID
    recall_list_query = []  # List to store recall for each document ID
    print(doc_ids)
    for doc_id in doc_ids:

        retrieved_count += 1

        precision = retrieved_count / doc_id
        recall = retrieved_count / top_k

        precision_list_query.append(precision)
        print(precision_list_query)
        recall_list_query.append(recall)
        print(recall_list_query)

    # Plot the precision-recall diagram for each query
    plt.plot(recall_list_query, precision_list_query, label=f'Document ID in Query {i+1}')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Top-{top_k} Precision-Recall Diagram for Query {i+1}')
    plt.show()
