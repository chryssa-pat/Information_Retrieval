import os
from collections import defaultdict
import math
import numpy as np
import pandas as pd
import sys
from matplotlib import pyplot as plt
from matplotlib.offsetbox import AnchoredText
inverted_index = defaultdict(list)

path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\docs"
os.chdir(path)
words_set = set()

document_count = {}  # Dictionary to store the count of documents each word appears in

for file in os.listdir(path):
    file_path = os.path.join(path, file)
    with open(file_path, 'r') as folder:
        text = folder.read()
        dictionary = text.split()
        words_set = words_set.union(set(dictionary))
        # Create a dictionary to store word counts for each document
        count = {}

        for word in dictionary:
            count[word] = count.get(word, 0) + 1

        # Update the inverted index with word counts for the current document
        for word, count in count.items():
            inverted_index[word].append((file, count))

                # Update the document count for the current word
            if word in document_count:
                document_count[word].add(file)
            else:
                document_count[word] = {file}

#print(inverted_index)


# Calculate IDF for each word and store it in a dictionary
idf_values = {}
for word, documents in document_count.items():
    idf = math.log10((len([f for f in os.listdir(path)])) / len(documents))
    idf_values[word] = idf

# Create a DataFrame with IDF values
idf_df = pd.DataFrame.from_dict(idf_values, orient='index', columns=['IDF'])

# Now you have a DataFrame with the IDF values for each word
print(idf_df)
csv_file_path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\idf.csv"

# Save the DataFrame to a CSV file
idf_df.to_csv(csv_file_path, index=False)

# Create a dictionary to store TF values for each word in each document
tf_values = {}
for word, occurrences in inverted_index.items():
    for document, count in occurrences:
        if document in tf_values:
            tf_values[document][word] = 1 + math.log10(count)
        else:
            tf_values[document] = {word: 1 + math.log10(count)}

# Create a DataFrame where rows are document titles and columns are words with their TF values
tf_df = pd.DataFrame.from_dict(tf_values, orient='index')
tf_df.fillna(0, inplace=True)  # Fill missing values with 0

# Now you have a DataFrame with TF values calculated as 1 + log(term_freq) for each word in each document
print(tf_df)

# Iterate through the TF and IDF DataFrames to calculate TF-IDF
tfidf_df = tf_df.copy()  # Create a copy of the TF DataFrame

# Multiply each TF value by the corresponding IDF value
for column in tfidf_df.columns:
    tfidf_df[column] = tfidf_df[column] * idf_df.loc[column, 'IDF']

# Now, tfidf_df contains the TF-IDF values
print(tfidf_df)

csv_file_path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\tfidf.csv"

# Save the DataFrame to a CSV file
tfidf_df.to_csv(csv_file_path, index=False)


query_path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\Queries_20"

# Read questions from the file
with open(query_path, 'r') as file:
    questions = file.readlines()


# Create a list of words from the inverted index
all_words = list(inverted_index.keys())

# Create a DataFrame with zeros
df = pd.DataFrame(0, index=range(len(questions)), columns=all_words)

# Iterate through questions and update the DataFrame
for i, question in enumerate(questions):
    words = question.upper().split()
    for word in words:
        if word in inverted_index:
            df.at[i, word] = 1

# Now, df is the DataFrame where each row represents a question, and columns represent words with values 1 or 0
print(df)
csv_file_path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\query.csv"

# Save the DataFrame to a CSV file
df.to_csv(csv_file_path, index=False)

idf_multiplied_df = df.multiply(idf_df['IDF'], axis=1)

print(idf_multiplied_df)

csv_file_path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\queryweights.csv"

# Save the DataFrame to a CSV file
idf_multiplied_df.to_csv(csv_file_path, index=False)

# Calculate the Euclidean norm for each row
euclidean_norms = np.linalg.norm(tfidf_df.values, axis=1)

# Create a DataFrame with the Euclidean norm values
euclidean_norms_df = pd.DataFrame({'Euclidean Norm': euclidean_norms}, index=tfidf_df.index)

# Now, euclidean_norms_df contains the Euclidean norm values for each row in tfidf_df
print(euclidean_norms_df)

# Calculate the Euclidean norm for each row
euclidean_norms_idf = np.linalg.norm(idf_multiplied_df.values, axis=1)

# Create a DataFrame with the Euclidean norm values
euclidean_norms_idf_df = pd.DataFrame({'Euclidean Norm': euclidean_norms_idf}, index=idf_multiplied_df.index)

# Now, euclidean_norms_idf_df contains the Euclidean norm values for each row in idf_multiplied_df
print(euclidean_norms_idf_df)

dot_products = np.dot(idf_multiplied_df.values, tfidf_df.values.T)

# Create a DataFrame with the dot product values
dot_product_df = pd.DataFrame(dot_products, columns=tfidf_df.index, index=idf_multiplied_df.index)

# Now, dot_product_df contains the dot product values for each pair of rows between idf_multiplied_df and tfidf_df
print(dot_product_df)

result_df = euclidean_norms_idf_df.values @ euclidean_norms_df.values.T

# Create a DataFrame with the result
result_df = pd.DataFrame(result_df, index=euclidean_norms_idf_df.index, columns=euclidean_norms_df.index)

# Now, result_df contains the result of multiplying euclidean_norms_idf_df with the transpose of euclidean_norms_df
print(result_df)

division_result_df = dot_product_df.div(result_df)


# Sort columns by converting column names to integers
division_result_df = division_result_df[sorted(division_result_df.columns, key=lambda x: int(x))]

# Save the sorted DataFrame to a CSV file
csv_file_path2 = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\cosine_similarity.csv"
division_result_df.to_csv(csv_file_path2, index=False)
print(division_result_df)
def sort_queries_all_docs(df):
    sorted_queries = pd.DataFrame(index=df.index, columns=['Sorted_Doc_IDs'])

    for query in df.index:
        sorted_values = df.loc[query].sort_values(ascending=False)
        sorted_doc_ids = list(sorted_values.index)

        sorted_queries.loc[query] = [sorted_doc_ids]

    return sorted_queries

# Sorting queries based on cosine similarity
sorted_queries_all_docs_df = sort_queries_all_docs(division_result_df)

# Display the result
print(sorted_queries_all_docs_df)

# Read relevant documents from file
relevant_documents_path = r"C:\Users\chryssa_pat\PycharmProjects\pythonProject\Relevant_20"
relevant_documents_dict = {}

# Read relevant documents from file
with open(relevant_documents_path, 'r') as rel_file:
    for query_id, line in enumerate(rel_file):
        rel_docs = [f"{int(doc):05d}" for doc in line.strip().split()]
        relevant_documents_dict[query_id] = rel_docs

# Initialize a list to store precision and recall values for each query
precision_recall_list = []
for query_id, row in sorted_queries_all_docs_df.iterrows():
    sorted_doc_ids = row['Sorted_Doc_IDs']

    # Retrieve relevant documents for the current query
    relevant_docs = relevant_documents_dict.get(query_id, [])

    # Compute relevance function X_Q (indexing starts with zero)
    X_Q = np.zeros(len(sorted_doc_ids), dtype=bool)
    for i, doc_id in enumerate(sorted_doc_ids):
        if doc_id in relevant_docs:
            X_Q[i] = True

    # Compute precision and recall values (indexing starts with zero)
    M = len(relevant_docs)
    P_Q = np.zeros(len(sorted_doc_ids))
    R_Q = np.zeros(len(sorted_doc_ids))
    for i, doc_id in enumerate(sorted_doc_ids):
        P_Q[i] = np.sum(X_Q[:i+1]) / (i+1) if (i+1) > 0 else 0
        R_Q[i] = np.sum(X_Q[:i+1]) / M

    precision_recall_list.append((P_Q, R_Q))

# Iterate through precision_recall_list and plot PR curves for each query
for query_id, (precision_values, recall_values) in enumerate(precision_recall_list, start=1):
    fig, ax = plt.subplots(1, 1, figsize=(6, 6))
    plt.plot(recall_values, precision_values, linestyle='--', marker='o', color='k', mfc='r')
    plt.title(f'Precision-Recall Curve - Query {query_id}')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.grid()
    plt.tight_layout()
    plt.show()
